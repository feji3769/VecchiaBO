{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"bo_loop.ipynbhttps://botorch.org/tutorials/turbo_1\n",
    "This notebook is a simply copy of the TuRBO loop from BoTorch with \n",
    "a Vecchia GP swapped in. See the link for details:\n",
    "https://botorch.org/tutorials/turbo_1\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# botorch imports\n",
    "from botorch.acquisition import qExpectedImprovement\n",
    "from botorch.generation import MaxPosteriorSampling\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.test_functions import Ackley\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "# gpytorch imports\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.means import ZeroMean\n",
    "\n",
    "# pyvecch imports\n",
    "from pyvecch.nbrs import ExactOracle, ApproximateOracle\n",
    "from pyvecch.models import RFVecchia\n",
    "from pyvecch.prediction import IndependentRF, VarianceCalibration\n",
    "from pyvecch.training import fit_model\n",
    "from pyvecch.input_transforms import Warping, Scaling, Identity\n",
    "\n",
    "tkwargs = {\"dtype\":torch.float, \"device\":\"cpu\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fun = Ackley(dim=20, negate=True).to(**tkwargs)\n",
    "fun.bounds[0, :].fill_(-5)\n",
    "fun.bounds[1, :].fill_(10)\n",
    "dim = fun.dim\n",
    "lb, ub = fun.bounds\n",
    "\n",
    "batch_size = 4\n",
    "n_init = 2 * dim\n",
    "\n",
    "def eval_objective(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return fun(unnormalize(x, fun.bounds))\n",
    "\n",
    "def get_initial_points(dim, n_pts, seed=0):\n",
    "    sobol = SobolEngine(dimension=dim, scramble=True, seed=seed)\n",
    "    X_init = sobol.draw(n=n_pts).to(**tkwargs)\n",
    "    return X_init\n",
    "\n",
    "\n",
    "def generate_batch(\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    batch_size,\n",
    "    n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "    num_restarts=10,\n",
    "    raw_samples=512,\n",
    "    acqf=\"ts\",  # \"ei\" or \"ts\"\n",
    "):\n",
    "    assert acqf in (\"ts\", \"ei\")\n",
    "    assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "    if n_candidates is None:\n",
    "        n_candidates = min(5000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "    # Scale the TR to be proportional to the lengthscales\n",
    "    x_center = X[Y.argmax(), :].clone()\n",
    "    weights = model.covar_module.base_kernel.lengthscale.squeeze().detach()\n",
    "    weights = weights / weights.mean()\n",
    "    weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "    tr_lb = torch.clamp(x_center - weights * state.length / 2.0, 0.0, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + weights * state.length / 2.0, 0.0, 1.0)\n",
    "\n",
    "    if acqf == \"ts\":\n",
    "        dim = X.shape[-1]\n",
    "        sobol = SobolEngine(dim, scramble=True)\n",
    "        pert = sobol.draw(n_candidates).to(**tkwargs)\n",
    "        pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "        # Create a perturbation mask\n",
    "        prob_perturb = min(20.0 / dim, 1.0)\n",
    "        mask = (\n",
    "            torch.rand(n_candidates, dim, **tkwargs)\n",
    "            <= prob_perturb\n",
    "        )\n",
    "        ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "        mask[ind, torch.randint(0, dim - 1, size=(len(ind),), device=tkwargs['device'])] = 1\n",
    "\n",
    "        # Create candidate points from the perturbations and the mask        \n",
    "        X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "        X_cand[mask] = pert[mask]\n",
    "\n",
    "        # Sample on the candidate points\n",
    "        thompson_sampling = MaxPosteriorSampling(model=model, replacement=False)\n",
    "        with torch.no_grad():  # We don't need gradients when using TS\n",
    "            X_next = thompson_sampling(X_cand, num_samples=batch_size)\n",
    "\n",
    "    elif acqf == \"ei\":\n",
    "        ei = qExpectedImprovement(model, train_Y.max(), maximize=True)\n",
    "        X_next, acq_value = optimize_acqf(\n",
    "            ei,\n",
    "            bounds=torch.stack([tr_lb, tr_ub]),\n",
    "            q=batch_size,\n",
    "            num_restarts=num_restarts,\n",
    "            raw_samples=raw_samples,\n",
    "        )\n",
    "\n",
    "    return X_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TurboState:\n",
    "    dim: int\n",
    "    batch_size: int\n",
    "    length: float = 0.8\n",
    "    length_min: float = 0.5 ** 7\n",
    "    length_max: float = 1.6\n",
    "    failure_counter: int = 0\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0\n",
    "    success_tolerance: int = 10  # Note: The original paper uses 3\n",
    "    best_value: float = -float(\"inf\")\n",
    "    restart_triggered: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.failure_tolerance = math.ceil(\n",
    "            max([4.0 / self.batch_size, float(self.dim) / self.batch_size])\n",
    "        )\n",
    "\n",
    "\n",
    "def update_state(state, Y_next):\n",
    "    if max(Y_next) > state.best_value + 1e-3 * math.fabs(state.best_value):\n",
    "        state.success_counter += 1\n",
    "        state.failure_counter = 0\n",
    "    else:\n",
    "        state.success_counter = 0\n",
    "        state.failure_counter += 1\n",
    "\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "\n",
    "    state.best_value = max(state.best_value, max(Y_next).item())\n",
    "    if state.length < state.length_min:\n",
    "        state.restart_triggered = True\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_initial_points(dim, n_init)\n",
    "y = torch.tensor(\n",
    "    [eval_objective(x_) for x_ in x], **tkwargs\n",
    ")\n",
    "\n",
    "state = TurboState(dim, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_settings = {\n",
    "    \"n_window\":50, \n",
    "    \"maxiter\":100, \n",
    "    \"rel_tol\":5e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NUM_RESTARTS = 10 \n",
    "RAW_SAMPLES = 512 \n",
    "N_CANDIDATES = min(5000, max(2000, 200 * dim)) \n",
    "\n",
    "\n",
    "while not state.restart_triggered:  # Run until TuRBO converges\n",
    "\n",
    "    n = x.shape[0]\n",
    "    m = int(7.2 * np.log10(n) ** 2)\n",
    "    train_batch_size = np.min([n, 128])\n",
    "\n",
    "    z = (y - y.mean()) / y.std()\n",
    "    # kernel, mean and likelihood can be swapped out, but prediction only \n",
    "    # expects a zero mean. \n",
    "    covar_module = ScaleKernel(MaternKernel(ard_num_dims = dim))\n",
    "    mean_module = ZeroMean()\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "\n",
    "    # We can get an approximate oracle with \n",
    "    # neighbor_oracle = ApproximateOracle(x,z,m,n_list = 100, n_probe = 75) \n",
    "    neighbor_oracle = ExactOracle(x,z,m)\n",
    "    # We can get variance inflation with \n",
    "    #prediction_stategy = VarianceCalibration(IndependentRF(), num_p = 2) \n",
    "    prediction_stategy = IndependentRF()\n",
    "    # We can get warping (or scaling) with \n",
    "    # input_transform = Warping(d = dim)\n",
    "    #input_transform = Scaling(d = dim)\n",
    "    input_transform = Identity(d = dim)\n",
    "    model = RFVecchia(covar_module, mean_module, likelihood, \n",
    "        neighbor_oracle, prediction_stategy, input_transform)\n",
    "\n",
    "\n",
    "    fit_model(\n",
    "        model,\n",
    "        train_batch_size = train_batch_size, \n",
    "        **training_settings\n",
    "    )\n",
    "    model.update_transform()\n",
    "    model.eval()\n",
    "    model.likelihood.eval()\n",
    "\n",
    "    # Create a batch\n",
    "    x_next = generate_batch(\n",
    "        state=state,\n",
    "        model=model,\n",
    "        X=x,\n",
    "        Y=z,\n",
    "        batch_size=batch_size,\n",
    "        n_candidates=N_CANDIDATES,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "        acqf=\"ts\",\n",
    "    ).squeeze(0)\n",
    "\n",
    "    y_next = torch.tensor(\n",
    "        [eval_objective(x_) for x_ in x_next], **tkwargs\n",
    "    )\n",
    "\n",
    "    # Update state\n",
    "    state = update_state(state=state, Y_next=y_next)\n",
    "\n",
    "    # Append data\n",
    "    x = torch.cat((x, x_next), dim=0)\n",
    "    y = torch.cat((y, y_next), dim=0)\n",
    "\n",
    "    # Print current status\n",
    "    print(\n",
    "        f\"{len(x)}) Best value: {state.best_value:.2e}, TR length: {state.length:.2e}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepVecchia",
   "language": "python",
   "name": "deepvecchia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
